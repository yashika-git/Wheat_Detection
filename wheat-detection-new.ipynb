{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing the requisite libraries \n\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\n\nfrom PIL import Image\n\nimport torch\nimport torchvision\n\nfrom torchvision import transforms \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\n\nDIR_INPUT = '/kaggle/input/global-wheat-detection'\nDIR_TRAIN = f'{DIR_INPUT}/train'\nDIR_TEST = f'{DIR_INPUT}/test'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-02T12:35:29.346377Z","iopub.execute_input":"2022-03-02T12:35:29.347105Z","iopub.status.idle":"2022-03-02T12:35:31.193104Z","shell.execute_reply.started":"2022-03-02T12:35:29.347014Z","shell.execute_reply":"2022-03-02T12:35:31.192374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/global-wheat-detection/train.csv')\nsubmit = pd.read_csv(\"/kaggle/input/global-wheat-detection/sample_submission.csv\")\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.194713Z","iopub.execute_input":"2022-03-02T12:35:31.194949Z","iopub.status.idle":"2022-03-02T12:35:31.48805Z","shell.execute_reply.started":"2022-03-02T12:35:31.194918Z","shell.execute_reply":"2022-03-02T12:35:31.487381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.489159Z","iopub.execute_input":"2022-03-02T12:35:31.48942Z","iopub.status.idle":"2022-03-02T12:35:31.50605Z","shell.execute_reply.started":"2022-03-02T12:35:31.489385Z","shell.execute_reply":"2022-03-02T12:35:31.505389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping unrequired columns\ntrain_df = train_df.drop(['width', 'height', 'source'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.509139Z","iopub.execute_input":"2022-03-02T12:35:31.509331Z","iopub.status.idle":"2022-03-02T12:35:31.520005Z","shell.execute_reply.started":"2022-03-02T12:35:31.509308Z","shell.execute_reply":"2022-03-02T12:35:31.518761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the number of unique images in train_df\ntrain_df['image_id'].nunique() ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.521298Z","iopub.execute_input":"2022-03-02T12:35:31.521617Z","iopub.status.idle":"2022-03-02T12:35:31.543314Z","shell.execute_reply.started":"2022-03-02T12:35:31.52158Z","shell.execute_reply":"2022-03-02T12:35:31.542398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the maximum number of bounding boxes present in an image\n(train_df['image_id'].value_counts()).max()  ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.546407Z","iopub.execute_input":"2022-03-02T12:35:31.546634Z","iopub.status.idle":"2022-03-02T12:35:31.572185Z","shell.execute_reply.started":"2022-03-02T12:35:31.546604Z","shell.execute_reply":"2022-03-02T12:35:31.571449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the minimum number of bounding boxes present in an image\n(train_df['image_id'].value_counts()).min()  ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.574779Z","iopub.execute_input":"2022-03-02T12:35:31.575239Z","iopub.status.idle":"2022-03-02T12:35:31.597367Z","shell.execute_reply.started":"2022-03-02T12:35:31.575207Z","shell.execute_reply":"2022-03-02T12:35:31.596578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bbox in each row is formatted [xmin, ymin, width, height]\n# Splitting the bounding box  \n\ntrain_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:31.59884Z","iopub.execute_input":"2022-03-02T12:35:31.599311Z","iopub.status.idle":"2022-03-02T12:35:32.961449Z","shell.execute_reply.started":"2022-03-02T12:35:31.599276Z","shell.execute_reply":"2022-03-02T12:35:32.96072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:32.962853Z","iopub.execute_input":"2022-03-02T12:35:32.963281Z","iopub.status.idle":"2022-03-02T12:35:32.977311Z","shell.execute_reply.started":"2022-03-02T12:35:32.963243Z","shell.execute_reply":"2022-03-02T12:35:32.976576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into test and validation sets\n\nimage_ids = train_df['image_id'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids[:-665]\n\nvalid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:32.980349Z","iopub.execute_input":"2022-03-02T12:35:32.980551Z","iopub.status.idle":"2022-03-02T12:35:33.02403Z","shell.execute_reply.started":"2022-03-02T12:35:32.980522Z","shell.execute_reply":"2022-03-02T12:35:33.023384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape, valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.02541Z","iopub.execute_input":"2022-03-02T12:35:33.02585Z","iopub.status.idle":"2022-03-02T12:35:33.031236Z","shell.execute_reply.started":"2022-03-02T12:35:33.025816Z","shell.execute_reply":"2022-03-02T12:35:33.030566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the numpy image to tensor\ntrans = transforms.Compose([transforms.ToTensor()])   ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.032604Z","iopub.execute_input":"2022-03-02T12:35:33.033146Z","iopub.status.idle":"2022-03-02T12:35:33.039007Z","shell.execute_reply.started":"2022-03-02T12:35:33.033095Z","shell.execute_reply":"2022-03-02T12:35:33.038028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a custom dataset\n\nclass WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None,train=True):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.train=train\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n        #Applying transformations\n        if self.transforms is not None:  \n            image = self.transforms(image)\n         \n         # For test data\n        if(self.train==False): \n            return image, image_id\n        \n        # For train and val data        \n           \n        # getting all the records for a particular image_id\n        records = self.df[self.df['image_id'] == image_id]   \n        \n     \n        # initially boxes = [xmin, ymin, width, height]\n        boxes = records[['x', 'y', 'w', 'h']].values\n        # converting width to xmax\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        # converting height to ymax\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n       \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # Now, boxes = [xmin, ymin, xmax, ymax]\n        # area = w*h => (ymax-ymin)*(xmax-xmin) \n        # creating a series of area\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # Since there is only 1 class present (wheat) in all the bboxes\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)   \n\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        return image, target, image_id ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.040307Z","iopub.execute_input":"2022-03-02T12:35:33.040732Z","iopub.status.idle":"2022-03-02T12:35:33.055537Z","shell.execute_reply.started":"2022-03-02T12:35:33.040699Z","shell.execute_reply":"2022-03-02T12:35:33.054762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = '/kaggle/input/global-wheat-detection/train'\ntest_dir = '/kaggle/input/global-wheat-detection/test'","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.058357Z","iopub.execute_input":"2022-03-02T12:35:33.058607Z","iopub.status.idle":"2022-03-02T12:35:33.067248Z","shell.execute_reply.started":"2022-03-02T12:35:33.058576Z","shell.execute_reply":"2022-03-02T12:35:33.066545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns the average loss\nclass Averager:       \n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n        \n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations     \n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n# collate function is being used because in one batch, we are passing 16 images, but the number of bbox in each of the 16 images may be different. To allow passing tensors of different sizes, collate function is used.        \n# * unpacks the iterable\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = WheatDataset(train_df, train_dir, trans, True)\nvalid_dataset = WheatDataset(valid_df, train_dir, trans, True)\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.068312Z","iopub.execute_input":"2022-03-02T12:35:33.06942Z","iopub.status.idle":"2022-03-02T12:35:33.093356Z","shell.execute_reply.started":"2022-03-02T12:35:33.069382Z","shell.execute_reply":"2022-03-02T12:35:33.092109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.094435Z","iopub.execute_input":"2022-03-02T12:35:33.094766Z","iopub.status.idle":"2022-03-02T12:35:33.144195Z","shell.execute_reply.started":"2022-03-02T12:35:33.094732Z","shell.execute_reply":"2022-03-02T12:35:33.1435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# images, targets, image_ids because Dataset class was returning these\nimages, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[4]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[4].permute(1,2,0).cpu().numpy().copy()  #Without using .copy(), there will be an error\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    #cv2.rectangle(image, start_point, end_point, color, thickness)\n    cv2.rectangle(sample,\n                  (int(box[0]), int(box[1])),\n                  (int(box[2]), int(box[3])),\n                  (220, 0, 0),3)\n    \n# ax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:33.145209Z","iopub.execute_input":"2022-03-02T12:35:33.145652Z","iopub.status.idle":"2022-03-02T12:35:42.135165Z","shell.execute_reply.started":"2022-03-02T12:35:33.145544Z","shell.execute_reply":"2022-03-02T12:35:42.134455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:42.136376Z","iopub.execute_input":"2022-03-02T12:35:42.137382Z","iopub.status.idle":"2022-03-02T12:35:49.267467Z","shell.execute_reply.started":"2022-03-02T12:35:42.137338Z","shell.execute_reply":"2022-03-02T12:35:49.266737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 class is for wheat and the other is the background\nnum_classes = 2  \n\n# Getting the number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nWEIGHTS_FILE = '../input/fasterrcnn/fasterrcnn_resnet50_fpn_best.pth'\n\n# loading the pre-trained weights\nmodel.load_state_dict(torch.load(WEIGHTS_FILE))  \n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:49.268851Z","iopub.execute_input":"2022-03-02T12:35:49.269079Z","iopub.status.idle":"2022-03-02T12:35:52.232186Z","shell.execute_reply.started":"2022-03-02T12:35:49.269047Z","shell.execute_reply":"2022-03-02T12:35:52.231444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\nmodel.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.00001)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n\nnum_epochs = 5\n\nloss_hist = Averager()\nitr = 1\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    for images, targets, image_ids in train_data_loader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)   ##Return the loss\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)  #Average out the loss\n\n        optimizer.zero_grad()\n        losses.backward()\n        # updating the parameters\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    # updating the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-02T12:35:52.233317Z","iopub.execute_input":"2022-03-02T12:35:52.233568Z","iopub.status.idle":"2022-03-02T13:04:59.078046Z","shell.execute_reply.started":"2022-03-02T12:35:52.233533Z","shell.execute_reply":"2022-03-02T13:04:59.077238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = WheatDataset(submit,test_dir,trans,False)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:04:59.080863Z","iopub.execute_input":"2022-03-02T13:04:59.081088Z","iopub.status.idle":"2022-03-02T13:04:59.086238Z","shell.execute_reply.started":"2022-03-02T13:04:59.081058Z","shell.execute_reply":"2022-03-02T13:04:59.08552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_loader = DataLoader( test_dataset, batch_size=8, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:04:59.087454Z","iopub.execute_input":"2022-03-02T13:04:59.087954Z","iopub.status.idle":"2022-03-02T13:04:59.096538Z","shell.execute_reply.started":"2022-03-02T13:04:59.08792Z","shell.execute_reply":"2022-03-02T13:04:59.095725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detection_threshold = 0.45","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:04:59.097916Z","iopub.execute_input":"2022-03-02T13:04:59.098159Z","iopub.status.idle":"2022-03-02T13:04:59.10526Z","shell.execute_reply.started":"2022-03-02T13:04:59.098125Z","shell.execute_reply":"2022-03-02T13:04:59.10444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining the format for storing prediction results\ndef format_prediction_string(boxes, scores): \n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:04:59.106474Z","iopub.execute_input":"2022-03-02T13:04:59.10686Z","iopub.status.idle":"2022-03-02T13:04:59.114148Z","shell.execute_reply.started":"2022-03-02T13:04:59.106824Z","shell.execute_reply":"2022-03-02T13:04:59.113243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Making the predictions\nresults=[]\nmodel.eval()\n \nfor images, image_ids in test_data_loader:\n    \n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n    \n    for i, image in enumerate(images):\n        boxes = outputs[i]['boxes'].data.cpu().numpy()  #[Xmin,Ymin,Xmax,Ymax]\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n        scores = scores[scores >= detection_threshold]\n        \n        image_id = image_ids[i]\n        \n        #converting the box format to [Xmin,Ymin,W,H]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1] \n        \n        #Storing the image id and boxes and scores in result dict\n        \n        result = {                                     \n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)}\n          \n        #Appending the result dict to results list\n        results.append(result)  ","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:04:59.11576Z","iopub.execute_input":"2022-03-02T13:04:59.116089Z","iopub.status.idle":"2022-03-02T13:05:00.061411Z","shell.execute_reply.started":"2022-03-02T13:04:59.116053Z","shell.execute_reply":"2022-03-02T13:05:00.060696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:05:00.062835Z","iopub.execute_input":"2022-03-02T13:05:00.063073Z","iopub.status.idle":"2022-03-02T13:05:00.074874Z","shell.execute_reply.started":"2022-03-02T13:05:00.06304Z","shell.execute_reply":"2022-03-02T13:05:00.074227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = images[1].permute(1,2,0).cpu().numpy().copy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\n\nboxes = boxes[scores >= detection_threshold].astype(np.int32)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:05:00.07603Z","iopub.execute_input":"2022-03-02T13:05:00.076573Z","iopub.status.idle":"2022-03-02T13:05:00.09279Z","shell.execute_reply.started":"2022-03-02T13:05:00.076538Z","shell.execute_reply":"2022-03-02T13:05:00.092091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting some of the predictions\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (int(box[0]), int(box[1])),\n                  (int(box[2]), int(box[3])),\n                  (220, 0, 0), 2)\nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:05:00.096383Z","iopub.execute_input":"2022-03-02T13:05:00.096577Z","iopub.status.idle":"2022-03-02T13:05:00.689215Z","shell.execute_reply.started":"2022-03-02T13:05:00.096554Z","shell.execute_reply":"2022-03-02T13:05:00.688553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:05:00.69022Z","iopub.execute_input":"2022-03-02T13:05:00.690572Z","iopub.status.idle":"2022-03-02T13:05:00.698752Z","shell.execute_reply.started":"2022-03-02T13:05:00.69054Z","shell.execute_reply":"2022-03-02T13:05:00.697929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}